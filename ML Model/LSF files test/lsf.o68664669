Sender: LSF System <lsfadmin@eu-ms-018-20>
Subject: Job 68664669: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-006-27> by user <morathba> in cluster <euler> at Sat Jul 14 17:20:21 2018.
Job was executed on host(s) <eu-ms-018-20>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:20:44 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:20:44 2018.
Terminated at Sat Jul 14 17:22:01 2018.
Results reported at Sat Jul 14 17:22:01 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   126.96 sec.
    Max Memory :                                 488 MB
    Average Memory :                             412.33 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               536.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   79 sec.
    Turnaround time :                            100 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:20:54.330284: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 237
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 57ms/step - loss: 21.6317
Epoch 2/200

16/16 [==============================] - 0s 20ms/step - loss: 18.4496
Epoch 3/200

16/16 [==============================] - 0s 20ms/step - loss: 15.6359
Epoch 4/200

16/16 [==============================] - 0s 20ms/step - loss: 13.1752
Epoch 5/200

16/16 [==============================] - 0s 19ms/step - loss: 11.0469
Epoch 6/200

16/16 [==============================] - 0s 20ms/step - loss: 9.2628
Epoch 7/200

16/16 [==============================] - 0s 20ms/step - loss: 7.6734
Epoch 8/200

16/16 [==============================] - 0s 20ms/step - loss: 6.3696
Epoch 9/200

16/16 [==============================] - 0s 20ms/step - loss: 5.2884
Epoch 10/200

16/16 [==============================] - 0s 19ms/step - loss: 4.4029
Epoch 11/200

16/16 [==============================] - 0s 20ms/step - loss: 3.6880
Epoch 12/200

16/16 [==============================] - 0s 20ms/step - loss: 3.1203
Epoch 13/200

16/16 [==============================] - 0s 20ms/step - loss: 2.6780
Epoch 14/200

16/16 [==============================] - 0s 20ms/step - loss: 2.3410
Epoch 15/200

16/16 [==============================] - 0s 20ms/step - loss: 2.0903
Epoch 16/200

16/16 [==============================] - 0s 20ms/step - loss: 1.9090
Epoch 17/200

16/16 [==============================] - 0s 21ms/step - loss: 1.7822
Epoch 18/200

16/16 [==============================] - 0s 20ms/step - loss: 1.6974
Epoch 19/200

16/16 [==============================] - 0s 20ms/step - loss: 1.6438
Epoch 20/200

16/16 [==============================] - 0s 20ms/step - loss: 1.6121
Epoch 21/200

16/16 [==============================] - 0s 21ms/step - loss: 1.5952
Epoch 22/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5872
Epoch 23/200

16/16 [==============================] - 0s 21ms/step - loss: 1.5838
Epoch 24/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5816
Epoch 25/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5783
Epoch 26/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5724
Epoch 27/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5630
Epoch 28/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5498
Epoch 29/200

16/16 [==============================] - 0s 21ms/step - loss: 1.5326
Epoch 30/200

16/16 [==============================] - 0s 20ms/step - loss: 1.5119
Epoch 31/200

16/16 [==============================] - 0s 20ms/step - loss: 1.4882
Epoch 32/200

16/16 [==============================] - 0s 21ms/step - loss: 1.4619
Epoch 33/200

16/16 [==============================] - 0s 21ms/step - loss: 1.4339
Epoch 34/200

16/16 [==============================] - 0s 21ms/step - loss: 1.4049
Epoch 35/200

16/16 [==============================] - 0s 20ms/step - loss: 1.3756
Epoch 36/200

16/16 [==============================] - 0s 20ms/step - loss: 1.3465
Epoch 37/200

16/16 [==============================] - 0s 20ms/step - loss: 1.3182
Epoch 38/200

16/16 [==============================] - 0s 20ms/step - loss: 1.2912
Epoch 39/200

16/16 [==============================] - 0s 21ms/step - loss: 1.2659
Epoch 40/200

16/16 [==============================] - 0s 21ms/step - loss: 1.2427
Epoch 41/200

16/16 [==============================] - 0s 20ms/step - loss: 1.2216
Epoch 42/200

16/16 [==============================] - 0s 21ms/step - loss: 1.2029
Epoch 43/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1866
Epoch 44/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1726
Epoch 45/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1607
Epoch 46/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1509
Epoch 47/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1430
Epoch 48/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1366
Epoch 49/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1316
Epoch 50/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1277
Epoch 51/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1246
Epoch 52/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1222
Epoch 53/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1203
Epoch 54/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1187
Epoch 55/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1172
Epoch 56/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1158
Epoch 57/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1144
Epoch 58/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1130
Epoch 59/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1114
Epoch 60/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1099
Epoch 61/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1082
Epoch 62/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1066
Epoch 63/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1050
Epoch 64/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1034
Epoch 65/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1019
Epoch 66/200

16/16 [==============================] - 0s 20ms/step - loss: 1.1005
Epoch 67/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0993
Epoch 68/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0982
Epoch 69/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0972
Epoch 70/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0963
Epoch 71/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0956
Epoch 72/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0950
Epoch 73/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0945
Epoch 74/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0941
Epoch 75/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0937
Epoch 76/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0934
Epoch 77/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0931
Epoch 78/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0929
Epoch 79/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0927
Epoch 80/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0925
Epoch 81/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0923
Epoch 82/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0921
Epoch 83/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0919
Epoch 84/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0918
Epoch 85/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0916
Epoch 86/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0915
Epoch 87/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0913
Epoch 88/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0912
Epoch 89/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0911
Epoch 90/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0910
Epoch 91/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0909
Epoch 92/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0909
Epoch 93/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0908
Epoch 94/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0907
Epoch 95/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0907
Epoch 96/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0906
Epoch 97/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0906
Epoch 98/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0906
Epoch 99/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0905
Epoch 100/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0905
Epoch 101/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0905
Epoch 102/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0904
Epoch 103/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0904
Epoch 104/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0904
Epoch 105/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0904
Epoch 106/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0904
Epoch 107/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 108/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 109/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 110/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 111/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 112/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 113/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 114/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0903
Epoch 115/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0903
Epoch 116/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0903
Epoch 117/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0903
Epoch 118/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0903
Epoch 119/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 120/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 121/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 122/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 123/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 124/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 125/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 126/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 127/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 128/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 129/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 130/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 131/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 132/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 133/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 134/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 135/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 136/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 137/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 138/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 139/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 140/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 141/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 142/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 143/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 144/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 145/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 146/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 147/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 148/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 149/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 150/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 151/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 152/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 153/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 154/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 155/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 156/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 157/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 158/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 159/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 160/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 161/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 162/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 163/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 164/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 165/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 166/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 167/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 168/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 169/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 170/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 171/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 172/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 173/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 174/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 175/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 176/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 177/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 178/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 179/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 180/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 181/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 182/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 183/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 184/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 185/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 186/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 187/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 188/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 189/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 190/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 191/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 192/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 193/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 194/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
Epoch 195/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 196/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 197/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 198/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 199/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0902
Epoch 200/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0902
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 89,890
Trainable params: 89,890
Non-trainable params: 0
_________________________________________________________________
None
Performance training set: 
[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.500 (+-0.00), recall: 0.000 (+-0.00), specificity: 1.000, precision: 0.000 (+-0.00) 

	Confusion matrix: 	 [4410    0] 
				 [934   0]



Performance test set: 
[0.5, 0.5, 0.5]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.500 (+-0.00), recall: 0.000 (+-0.00), specificity: 1.000, precision: 0.000 (+-0.00) 

	Confusion matrix: 	 [830   0] 
				 [115   0]



Time elapsed: 71.75112843513489
