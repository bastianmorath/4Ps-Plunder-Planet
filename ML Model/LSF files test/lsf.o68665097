Sender: LSF System <lsfadmin@eu-ms-007-25>
Subject: Job 68665097: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-004-17> by user <morathba> in cluster <euler> at Sat Jul 14 17:21:52 2018.
Job was executed on host(s) <eu-ms-007-25>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:22:16 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:22:16 2018.
Terminated at Sat Jul 14 17:23:36 2018.
Results reported at Sat Jul 14 17:23:36 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   140.83 sec.
    Max Memory :                                 398 MB
    Average Memory :                             303.00 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               626.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   80 sec.
    Turnaround time :                            104 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:22:26.611272: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 281
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 55ms/step - loss: 1.0650
Epoch 2/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0593
Epoch 3/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0484
Epoch 4/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0367
Epoch 5/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0505
Epoch 6/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0384
Epoch 7/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0280
Epoch 8/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0253
Epoch 9/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0274
Epoch 10/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0110
Epoch 11/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0065
Epoch 12/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0131
Epoch 13/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0100
Epoch 14/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0000
Epoch 15/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0074
Epoch 16/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0022
Epoch 17/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0033
Epoch 18/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0069
Epoch 19/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0004
Epoch 20/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9970
Epoch 21/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9987
Epoch 22/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9922
Epoch 23/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9945
Epoch 24/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9944
Epoch 25/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9914
Epoch 26/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9908
Epoch 27/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9901
Epoch 28/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9886
Epoch 29/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9852
Epoch 30/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9846
Epoch 31/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9832
Epoch 32/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9841
Epoch 33/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9872
Epoch 34/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9815
Epoch 35/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9809
Epoch 36/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9818
Epoch 37/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9783
Epoch 38/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9816
Epoch 39/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9750
Epoch 40/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9818
Epoch 41/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9771
Epoch 42/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9801
Epoch 43/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9829
Epoch 44/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9806
Epoch 45/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9782
Epoch 46/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9749
Epoch 47/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9781
Epoch 48/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9685
Epoch 49/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9717
Epoch 50/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9736
Epoch 51/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9709
Epoch 52/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9702
Epoch 53/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9759
Epoch 54/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9746
Epoch 55/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9816
Epoch 56/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9782
Epoch 57/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9725
Epoch 58/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9751
Epoch 59/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9700
Epoch 60/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9710
Epoch 61/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9714
Epoch 62/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9760
Epoch 63/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9700
Epoch 64/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9671
Epoch 65/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9705
Epoch 66/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9678
Epoch 67/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9599
Epoch 68/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9607
Epoch 69/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9612
Epoch 70/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9618
Epoch 71/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9570
Epoch 72/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9623
Epoch 73/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9545
Epoch 74/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9603
Epoch 75/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9568
Epoch 76/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9464
Epoch 77/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9464
Epoch 78/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9425
Epoch 79/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9474
Epoch 80/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9435
Epoch 81/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9516
Epoch 82/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9520
Epoch 83/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9396
Epoch 84/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9430
Epoch 85/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9383
Epoch 86/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9879
Epoch 87/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9740
Epoch 88/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9944
Epoch 89/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9783
Epoch 90/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9853
Epoch 91/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9812
Epoch 92/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9735
Epoch 93/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9808
Epoch 94/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9802
Epoch 95/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9776
Epoch 96/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9741
Epoch 97/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9808
Epoch 98/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9817
Epoch 99/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9699
Epoch 100/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9714
Epoch 101/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9676
Epoch 102/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9738
Epoch 103/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9646
Epoch 104/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9650
Epoch 105/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9633
Epoch 106/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9605
Epoch 107/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9617
Epoch 108/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9514
Epoch 109/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9585
Epoch 110/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9555
Epoch 111/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9514
Epoch 112/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9577
Epoch 113/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9452
Epoch 114/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9594
Epoch 115/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9535
Epoch 116/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9507
Epoch 117/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9515
Epoch 118/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9505
Epoch 119/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9524
Epoch 120/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9520
Epoch 121/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9498
Epoch 122/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9498
Epoch 123/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9445
Epoch 124/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9454
Epoch 125/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9438
Epoch 126/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9443
Epoch 127/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9343
Epoch 128/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9401
Epoch 129/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9315
Epoch 130/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9314
Epoch 131/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9357
Epoch 132/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9352
Epoch 133/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9271
Epoch 134/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9322
Epoch 135/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9340
Epoch 136/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9341
Epoch 137/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9214
Epoch 138/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9360
Epoch 139/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9206
Epoch 140/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9277
Epoch 141/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9201
Epoch 142/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9142
Epoch 143/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9269
Epoch 144/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9238
Epoch 145/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9256
Epoch 146/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9158
Epoch 147/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9189
Epoch 148/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9184
Epoch 149/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9095
Epoch 150/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9099
Epoch 151/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9082
Epoch 152/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9173
Epoch 153/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9066
Epoch 154/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8934
Epoch 155/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9038
Epoch 156/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8886
Epoch 157/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8782
Epoch 158/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8854
Epoch 159/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9014
Epoch 160/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9088
Epoch 161/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9222
Epoch 162/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9385
Epoch 163/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9626
Epoch 164/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9596
Epoch 165/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9451
Epoch 166/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9469
Epoch 167/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9518
Epoch 168/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9518
Epoch 169/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9489
Epoch 170/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9416
Epoch 171/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9455
Epoch 172/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9384
Epoch 173/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9395
Epoch 174/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9373
Epoch 175/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9368
Epoch 176/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9269
Epoch 177/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9237
Epoch 178/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9301
Epoch 179/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9208
Epoch 180/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9101
Epoch 181/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9244
Epoch 182/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9184
Epoch 183/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9111
Epoch 184/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9092
Epoch 185/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9092
Epoch 186/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9013
Epoch 187/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9022
Epoch 188/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9013
Epoch 189/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9045
Epoch 190/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8914
Epoch 191/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8930
Epoch 192/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8970
Epoch 193/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8855
Epoch 194/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8824
Epoch 195/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8884
Epoch 196/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8709
Epoch 197/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8800
Epoch 198/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8872
Epoch 199/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8854
Epoch 200/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8783
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 334, 128)          0         
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 334, 96)           0         
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 334, 64)           0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 89,890
Trainable params: 89,890
Non-trainable params: 0
_________________________________________________________________
None
Performance training set: 
[0.6339174674957604, 0.6173179892911373, 0.627347051894963, 0.7467953667953668, 0.7227541827541827, 0.6360483042137719, 0.6643281743694103, 0.5846300846300846, 0.6568914956011731, 0.7384490061909417, 0.7336655592469545, 0.6129978177850518, 0.6440281030444964, 0.6600060096153846, 0.6671926408379776, 0.49894323671497587]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.653 (+-0.06), recall: 0.574 (+-0.18), specificity: 0.732, precision: 0.298 (+-0.07) 

	Confusion matrix: 	 [3292 1182] 
				 [341 529]



Performance test set: 
[0.6538701067615659, 0.5119347664936991, 0.5718118686868687]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.579 (+-0.06), recall: 0.376 (+-0.11), specificity: 0.782, precision: 0.301 (+-0.10) 

	Confusion matrix: 	 [553 158] 
				 [120  59]



Time elapsed: 73.84546256065369
