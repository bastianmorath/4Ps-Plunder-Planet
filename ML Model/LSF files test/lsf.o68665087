Sender: LSF System <lsfadmin@eu-ms-014-22>
Subject: Job 68665087: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-002-06> by user <morathba> in cluster <euler> at Sat Jul 14 17:21:51 2018.
Job was executed on host(s) <eu-ms-014-22>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:22:16 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:22:16 2018.
Terminated at Sat Jul 14 17:23:37 2018.
Results reported at Sat Jul 14 17:23:37 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   132.39 sec.
    Max Memory :                                 505 MB
    Average Memory :                             418.33 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               519.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   80 sec.
    Turnaround time :                            106 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:22:25.965993: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 237
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 54ms/step - loss: 1.0932
Epoch 2/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0846
Epoch 3/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0732
Epoch 4/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1596
Epoch 5/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0650
Epoch 6/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0773
Epoch 7/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0661
Epoch 8/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0609
Epoch 9/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0587
Epoch 10/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0581
Epoch 11/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0544
Epoch 12/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0430
Epoch 13/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0461
Epoch 14/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0383
Epoch 15/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0420
Epoch 16/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0330
Epoch 17/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0307
Epoch 18/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0283
Epoch 19/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0326
Epoch 20/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0332
Epoch 21/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0198
Epoch 22/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0230
Epoch 23/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0235
Epoch 24/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0245
Epoch 25/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0235
Epoch 26/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0208
Epoch 27/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0303
Epoch 28/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0219
Epoch 29/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0214
Epoch 30/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0134
Epoch 31/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0178
Epoch 32/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0124
Epoch 33/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0252
Epoch 34/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0285
Epoch 35/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0280
Epoch 36/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0159
Epoch 37/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0267
Epoch 38/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0187
Epoch 39/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0155
Epoch 40/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0140
Epoch 41/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0152
Epoch 42/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0179
Epoch 43/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0136
Epoch 44/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0151
Epoch 45/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0163
Epoch 46/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0162
Epoch 47/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0065
Epoch 48/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0142
Epoch 49/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0057
Epoch 50/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0072
Epoch 51/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0068
Epoch 52/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0035
Epoch 53/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0015
Epoch 54/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0045
Epoch 55/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0004
Epoch 56/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0020
Epoch 57/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0043
Epoch 58/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9983
Epoch 59/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0007
Epoch 60/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0005
Epoch 61/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9979
Epoch 62/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0034
Epoch 63/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9955
Epoch 64/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9973
Epoch 65/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9961
Epoch 66/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9927
Epoch 67/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9950
Epoch 68/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9930
Epoch 69/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9928
Epoch 70/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9978
Epoch 71/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9968
Epoch 72/200

16/16 [==============================] - 0s 22ms/step - loss: 1.0013
Epoch 73/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9875
Epoch 74/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9881
Epoch 75/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9910
Epoch 76/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9916
Epoch 77/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9892
Epoch 78/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9893
Epoch 79/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9983
Epoch 80/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9892
Epoch 81/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9795
Epoch 82/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9826
Epoch 83/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9860
Epoch 84/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9824
Epoch 85/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9844
Epoch 86/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9754
Epoch 87/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9844
Epoch 88/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9753
Epoch 89/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9780
Epoch 90/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9755
Epoch 91/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9744
Epoch 92/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9729
Epoch 93/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9952
Epoch 94/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9809
Epoch 95/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9864
Epoch 96/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9848
Epoch 97/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9832
Epoch 98/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9823
Epoch 99/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9782
Epoch 100/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9731
Epoch 101/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9681
Epoch 102/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9650
Epoch 103/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9743
Epoch 104/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9597
Epoch 105/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9685
Epoch 106/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9754
Epoch 107/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9578
Epoch 108/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9577
Epoch 109/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9579
Epoch 110/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9561
Epoch 111/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9651
Epoch 112/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9487
Epoch 113/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9545
Epoch 114/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9518
Epoch 115/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9488
Epoch 116/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9498
Epoch 117/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9477
Epoch 118/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9405
Epoch 119/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9418
Epoch 120/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9366
Epoch 121/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9487
Epoch 122/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9421
Epoch 123/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9362
Epoch 124/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9322
Epoch 125/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9258
Epoch 126/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9946
Epoch 127/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9586
Epoch 128/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9853
Epoch 129/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9734
Epoch 130/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9717
Epoch 131/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9716
Epoch 132/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9602
Epoch 133/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9663
Epoch 134/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9649
Epoch 135/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9626
Epoch 136/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9595
Epoch 137/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9526
Epoch 138/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9526
Epoch 139/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9461
Epoch 140/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9477
Epoch 141/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9361
Epoch 142/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9350
Epoch 143/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9408
Epoch 144/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9263
Epoch 145/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9270
Epoch 146/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9322
Epoch 147/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9250
Epoch 148/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9237
Epoch 149/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9260
Epoch 150/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9199
Epoch 151/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9145
Epoch 152/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9158
Epoch 153/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9062
Epoch 154/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8930
Epoch 155/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9050
Epoch 156/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9021
Epoch 157/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9090
Epoch 158/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8999
Epoch 159/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8931
Epoch 160/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8931
Epoch 161/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8851
Epoch 162/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8859
Epoch 163/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8866
Epoch 164/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8831
Epoch 165/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8775
Epoch 166/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8778
Epoch 167/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8705
Epoch 168/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8887
Epoch 169/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8600
Epoch 170/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8651
Epoch 171/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8497
Epoch 172/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8642
Epoch 173/200

16/16 [==============================] - 0s 21ms/step - loss: 0.8597
Epoch 174/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8570
Epoch 175/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8653
Epoch 176/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8561
Epoch 177/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8604
Epoch 178/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8500
Epoch 179/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8342
Epoch 180/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8273
Epoch 181/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8209
Epoch 182/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8239
Epoch 183/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7996
Epoch 184/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8104
Epoch 185/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8188
Epoch 186/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8291
Epoch 187/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8169
Epoch 188/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8197
Epoch 189/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8068
Epoch 190/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8385
Epoch 191/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8131
Epoch 192/200

16/16 [==============================] - 0s 22ms/step - loss: 0.8013
Epoch 193/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7927
Epoch 194/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7926
Epoch 195/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7911
Epoch 196/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7879
Epoch 197/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7837
Epoch 198/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7916
Epoch 199/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7739
Epoch 200/200

16/16 [==============================] - 0s 22ms/step - loss: 0.7797
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 334, 128)          0         
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 334, 96)           0         
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 334, 64)           0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 89,890
Trainable params: 89,890
Non-trainable params: 0
_________________________________________________________________
None
Performance training set: 
[0.7740306288693385, 0.7162769784172662, 0.8373040752351097, 0.8518707482993197, 0.761928988378943, 0.8336422136422137, 0.5886148425456947, 0.8307592049527535, 0.7371794871794872, 0.7369843271482617, 0.7523399758454106, 0.7757671389243269, 0.8164478764478765, 0.750886524822695, 0.6802466569666736, 0.7047047047047048]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.759 (+-0.07), recall: 0.814 (+-0.11), specificity: 0.704, precision: 0.386 (+-0.07) 

	Confusion matrix: 	 [3126 1274] 
				 [159 785]



Performance test set: 
[0.560077519379845, 0.4745078522450785, 0.5496551724137931]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.528 (+-0.04), recall: 0.375 (+-0.06), specificity: 0.681, precision: 0.135 (+-0.04) 

	Confusion matrix: 	 [550 257] 
				 [65 40]



Time elapsed: 75.62179660797119
