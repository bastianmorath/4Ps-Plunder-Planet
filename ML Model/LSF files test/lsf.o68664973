Sender: LSF System <lsfadmin@eu-ms-006-04>
Subject: Job 68664973: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-027-16> by user <morathba> in cluster <euler> at Sat Jul 14 17:21:19 2018.
Job was executed on host(s) <eu-ms-006-04>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:21:46 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:21:46 2018.
Terminated at Sat Jul 14 17:23:06 2018.
Results reported at Sat Jul 14 17:23:06 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   129.92 sec.
    Max Memory :                                 509 MB
    Average Memory :                             414.67 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               515.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   83 sec.
    Turnaround time :                            107 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:21:57.112717: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 237
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 54ms/step - loss: 1.0733
Epoch 2/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0633
Epoch 3/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0974
Epoch 4/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0652
Epoch 5/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0479
Epoch 6/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0460
Epoch 7/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0373
Epoch 8/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0259
Epoch 9/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0229
Epoch 10/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0337
Epoch 11/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0164
Epoch 12/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0143
Epoch 13/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0043
Epoch 14/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0055
Epoch 15/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0052
Epoch 16/200

16/16 [==============================] - 0s 20ms/step - loss: 1.0007
Epoch 17/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9987
Epoch 18/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0019
Epoch 19/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9917
Epoch 20/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9926
Epoch 21/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9958
Epoch 22/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9883
Epoch 23/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9836
Epoch 24/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9889
Epoch 25/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9827
Epoch 26/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9856
Epoch 27/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9835
Epoch 28/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9831
Epoch 29/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9796
Epoch 30/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9770
Epoch 31/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9788
Epoch 32/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9791
Epoch 33/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9795
Epoch 34/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9789
Epoch 35/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9795
Epoch 36/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0807
Epoch 37/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0109
Epoch 38/200

16/16 [==============================] - 0s 20ms/step - loss: 0.9948
Epoch 39/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0016
Epoch 40/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0062
Epoch 41/200

16/16 [==============================] - 0s 21ms/step - loss: 1.0017
Epoch 42/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9961
Epoch 43/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9880
Epoch 44/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9884
Epoch 45/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9888
Epoch 46/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9890
Epoch 47/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9893
Epoch 48/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9883
Epoch 49/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9888
Epoch 50/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9837
Epoch 51/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9919
Epoch 52/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9855
Epoch 53/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9812
Epoch 54/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9806
Epoch 55/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9846
Epoch 56/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9834
Epoch 57/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9890
Epoch 58/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9809
Epoch 59/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9820
Epoch 60/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9808
Epoch 61/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9753
Epoch 62/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9811
Epoch 63/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9786
Epoch 64/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9776
Epoch 65/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9807
Epoch 66/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9674
Epoch 67/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9704
Epoch 68/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9768
Epoch 69/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9703
Epoch 70/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9717
Epoch 71/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9717
Epoch 72/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9739
Epoch 73/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9663
Epoch 74/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9718
Epoch 75/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9653
Epoch 76/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9753
Epoch 77/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9648
Epoch 78/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9699
Epoch 79/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9642
Epoch 80/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9686
Epoch 81/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9642
Epoch 82/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9607
Epoch 83/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9606
Epoch 84/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9594
Epoch 85/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9610
Epoch 86/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9583
Epoch 87/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9577
Epoch 88/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9630
Epoch 89/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9656
Epoch 90/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9661
Epoch 91/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9553
Epoch 92/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9604
Epoch 93/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9636
Epoch 94/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9592
Epoch 95/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9640
Epoch 96/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9667
Epoch 97/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9713
Epoch 98/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9735
Epoch 99/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9662
Epoch 100/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9711
Epoch 101/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9648
Epoch 102/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9626
Epoch 103/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9660
Epoch 104/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9702
Epoch 105/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9636
Epoch 106/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9578
Epoch 107/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9583
Epoch 108/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9601
Epoch 109/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9601
Epoch 110/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9514
Epoch 111/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9567
Epoch 112/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9509
Epoch 113/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9561
Epoch 114/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9521
Epoch 115/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9460
Epoch 116/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9492
Epoch 117/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9528
Epoch 118/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9549
Epoch 119/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9381
Epoch 120/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9431
Epoch 121/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9433
Epoch 122/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9410
Epoch 123/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9359
Epoch 124/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9402
Epoch 125/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9380
Epoch 126/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9328
Epoch 127/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9382
Epoch 128/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9316
Epoch 129/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9333
Epoch 130/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9361
Epoch 131/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9221
Epoch 132/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9231
Epoch 133/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9774
Epoch 134/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9835
Epoch 135/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9869
Epoch 136/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9774
Epoch 137/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9764
Epoch 138/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9646
Epoch 139/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9790
Epoch 140/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9677
Epoch 141/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9639
Epoch 142/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9641
Epoch 143/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9655
Epoch 144/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9588
Epoch 145/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9529
Epoch 146/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9519
Epoch 147/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9488
Epoch 148/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9599
Epoch 149/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9568
Epoch 150/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9458
Epoch 151/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9483
Epoch 152/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9406
Epoch 153/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9411
Epoch 154/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9391
Epoch 155/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9389
Epoch 156/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9295
Epoch 157/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9315
Epoch 158/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9275
Epoch 159/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9353
Epoch 160/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9293
Epoch 161/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9296
Epoch 162/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9244
Epoch 163/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9180
Epoch 164/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9249
Epoch 165/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9150
Epoch 166/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9282
Epoch 167/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9125
Epoch 168/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9291
Epoch 169/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9116
Epoch 170/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9200
Epoch 171/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9071
Epoch 172/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9073
Epoch 173/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9082
Epoch 174/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9124
Epoch 175/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9050
Epoch 176/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9120
Epoch 177/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9196
Epoch 178/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9319
Epoch 179/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9230
Epoch 180/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9350
Epoch 181/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9251
Epoch 182/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9160
Epoch 183/200

16/16 [==============================] - 0s 21ms/step - loss: 1.1140
Epoch 184/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9632
Epoch 185/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9547
Epoch 186/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9439
Epoch 187/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9608
Epoch 188/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9612
Epoch 189/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9426
Epoch 190/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9441
Epoch 191/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9442
Epoch 192/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9363
Epoch 193/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9335
Epoch 194/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9405
Epoch 195/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9278
Epoch 196/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9282
Epoch 197/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9143
Epoch 198/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9248
Epoch 199/200

16/16 [==============================] - 0s 21ms/step - loss: 0.9150
Epoch 200/200

16/16 [==============================] - 0s 22ms/step - loss: 0.9120
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 334, 128)          0         
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 334, 96)           0         
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 334, 64)           0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 89,890
Trainable params: 89,890
Non-trainable params: 0
_________________________________________________________________
None
Performance training set: 
[0.6384560057540157, 0.6164757228587016, 0.7203975236233302, 0.4989209425236732, 0.6339285714285714, 0.747874149659864, 0.6171477534407969, 0.579861111111111, 0.5731486715658564, 0.6422816088095078, 0.7722742373905165, 0.6862548262548263, 0.6359675480769231, 0.685304659498208, 0.6520376175548589, 0.7618790218790219]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.654 (+-0.07), recall: 0.656 (+-0.17), specificity: 0.652, precision: 0.295 (+-0.07) 

	Confusion matrix: 	 [2946 1498] 
				 [271 629]



Performance test set: 
[0.5690999291282778, 0.5802558283474314, 0.5586130627114234]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.569 (+-0.01), recall: 0.408 (+-0.04), specificity: 0.730, precision: 0.229 (+-0.02) 

	Confusion matrix: 	 [558 205] 
				 [87 62]



Time elapsed: 74.52128672599792
