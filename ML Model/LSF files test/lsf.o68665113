Sender: LSF System <lsfadmin@eu-ms-018-05>
Subject: Job 68665113: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-005-31> by user <morathba> in cluster <euler> at Sat Jul 14 17:22:22 2018.
Job was executed on host(s) <eu-ms-018-05>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:22:44 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:22:44 2018.
Terminated at Sat Jul 14 17:24:12 2018.
Results reported at Sat Jul 14 17:24:12 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   142.19 sec.
    Max Memory :                                 379 MB
    Average Memory :                             319.00 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               645.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   94 sec.
    Turnaround time :                            110 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:22:55.192030: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 237
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 87ms/step - loss: 1.7571
Epoch 2/200

16/16 [==============================] - 0s 23ms/step - loss: 2.0886
Epoch 3/200

16/16 [==============================] - 0s 23ms/step - loss: 1.3367
Epoch 4/200

16/16 [==============================] - 0s 23ms/step - loss: 1.1076
Epoch 5/200

16/16 [==============================] - 0s 23ms/step - loss: 1.0598
Epoch 6/200

16/16 [==============================] - 0s 23ms/step - loss: 1.0108
Epoch 7/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9884
Epoch 8/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9826
Epoch 9/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9753
Epoch 10/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9701
Epoch 11/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9669
Epoch 12/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9617
Epoch 13/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9602
Epoch 14/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9551
Epoch 15/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9514
Epoch 16/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9490
Epoch 17/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9464
Epoch 18/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9429
Epoch 19/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9407
Epoch 20/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9381
Epoch 21/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9338
Epoch 22/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9317
Epoch 23/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9285
Epoch 24/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9243
Epoch 25/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9236
Epoch 26/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9281
Epoch 27/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9213
Epoch 28/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9213
Epoch 29/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9175
Epoch 30/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9126
Epoch 31/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9118
Epoch 32/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9065
Epoch 33/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9022
Epoch 34/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8976
Epoch 35/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8946
Epoch 36/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8931
Epoch 37/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8905
Epoch 38/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8841
Epoch 39/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8774
Epoch 40/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8801
Epoch 41/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8720
Epoch 42/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8682
Epoch 43/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8609
Epoch 44/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8607
Epoch 45/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8547
Epoch 46/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8467
Epoch 47/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8430
Epoch 48/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8360
Epoch 49/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8334
Epoch 50/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8293
Epoch 51/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8552
Epoch 52/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8439
Epoch 53/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8346
Epoch 54/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8264
Epoch 55/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8169
Epoch 56/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8110
Epoch 57/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8080
Epoch 58/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8006
Epoch 59/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8006
Epoch 60/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7832
Epoch 61/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7830
Epoch 62/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7753
Epoch 63/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7604
Epoch 64/200

16/16 [==============================] - 0s 24ms/step - loss: 0.7576
Epoch 65/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7636
Epoch 66/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8133
Epoch 67/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8108
Epoch 68/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8030
Epoch 69/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7874
Epoch 70/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7818
Epoch 71/200

16/16 [==============================] - 0s 24ms/step - loss: 0.7580
Epoch 72/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7552
Epoch 73/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7481
Epoch 74/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7351
Epoch 75/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7206
Epoch 76/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7080
Epoch 77/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7016
Epoch 78/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6912
Epoch 79/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6736
Epoch 80/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6688
Epoch 81/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6658
Epoch 82/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6803
Epoch 83/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6950
Epoch 84/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7818
Epoch 85/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7060
Epoch 86/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7069
Epoch 87/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6973
Epoch 88/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6712
Epoch 89/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6739
Epoch 90/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6545
Epoch 91/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6311
Epoch 92/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6305
Epoch 93/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6039
Epoch 94/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6140
Epoch 95/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5816
Epoch 96/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5783
Epoch 97/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5737
Epoch 98/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5749
Epoch 99/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5586
Epoch 100/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5813
Epoch 101/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5425
Epoch 102/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5266
Epoch 103/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5166
Epoch 104/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5601
Epoch 105/200

16/16 [==============================] - 0s 24ms/step - loss: 0.7649
Epoch 106/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6217
Epoch 107/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5485
Epoch 108/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5637
Epoch 109/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5327
Epoch 110/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5308
Epoch 111/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4902
Epoch 112/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4881
Epoch 113/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4682
Epoch 114/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4648
Epoch 115/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4505
Epoch 116/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4461
Epoch 117/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4350
Epoch 118/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4367
Epoch 119/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4389
Epoch 120/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4243
Epoch 121/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4219
Epoch 122/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3881
Epoch 123/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3709
Epoch 124/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3564
Epoch 125/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3585
Epoch 126/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3613
Epoch 127/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4296
Epoch 128/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6147
Epoch 129/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9077
Epoch 130/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6991
Epoch 131/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6932
Epoch 132/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6238
Epoch 133/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6606
Epoch 134/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5656
Epoch 135/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5640
Epoch 136/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5316
Epoch 137/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5420
Epoch 138/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5046
Epoch 139/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4822
Epoch 140/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4614
Epoch 141/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4517
Epoch 142/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4375
Epoch 143/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4068
Epoch 144/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3940
Epoch 145/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3822
Epoch 146/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3713
Epoch 147/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3579
Epoch 148/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3483
Epoch 149/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3338
Epoch 150/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3188
Epoch 151/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3083
Epoch 152/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2970
Epoch 153/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2815
Epoch 154/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2725
Epoch 155/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2651
Epoch 156/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2524
Epoch 157/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2435
Epoch 158/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2332
Epoch 159/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2243
Epoch 160/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2165
Epoch 161/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2092
Epoch 162/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2081
Epoch 163/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2218
Epoch 164/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2680
Epoch 165/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2198
Epoch 166/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2082
Epoch 167/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1965
Epoch 168/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1964
Epoch 169/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1902
Epoch 170/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1732
Epoch 171/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1711
Epoch 172/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1745
Epoch 173/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1603
Epoch 174/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1594
Epoch 175/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1731
Epoch 176/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1583
Epoch 177/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1573
Epoch 178/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1337
Epoch 179/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1456
Epoch 180/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1359
Epoch 181/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1262
Epoch 182/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1109
Epoch 183/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1179
Epoch 184/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1008
Epoch 185/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1069
Epoch 186/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0924
Epoch 187/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0911
Epoch 188/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0894
Epoch 189/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0849
Epoch 190/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0797
Epoch 191/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0772
Epoch 192/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0737
Epoch 193/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0693
Epoch 194/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0687
Epoch 195/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0624
Epoch 196/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0606
Epoch 197/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0590
Epoch 198/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0542
Epoch 199/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0531
Epoch 200/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0490
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 334, 128)          512       
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 334, 96)           384       
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 334, 64)           256       
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 91,042
Trainable params: 90,466
Non-trainable params: 576
_________________________________________________________________
None
Performance training set: 
[0.6030649038461539, 0.5409815005138747, 0.4909353387497482, 0.5983043382514865, 0.5551724137931034, 0.5605896835405032, 0.5087516087516087, 0.513326816552623, 0.5578968903436988, 0.5584533818882879, 0.5151515151515151, 0.5222675120772947, 0.5171768707482993, 0.5522727272727274, 0.5621049201694363, 0.6196138996138996]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.549 (+-0.04), recall: 0.196 (+-0.11), specificity: 0.901, precision: 0.354 (+-0.21) 

	Confusion matrix: 	 [3990  436] 
				 [725 193]



Performance test set: 
[0.4840538625088589, 0.47880132040437384, 0.43023255813953487]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.464 (+-0.02), recall: 0.190 (+-0.08), specificity: 0.739, precision: 0.109 (+-0.03) 

	Confusion matrix: 	 [567 202] 
				 [107  24]



Time elapsed: 82.86569094657898
