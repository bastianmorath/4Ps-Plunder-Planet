Sender: LSF System <lsfadmin@eu-ms-006-15>
Subject: Job 68665138: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-008-24> by user <morathba> in cluster <euler> at Sat Jul 14 17:23:22 2018.
Job was executed on host(s) <eu-ms-006-15>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:23:47 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:23:47 2018.
Terminated at Sat Jul 14 17:25:46 2018.
Results reported at Sat Jul 14 17:25:46 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   199.07 sec.
    Max Memory :                                 569 MB
    Average Memory :                             469.50 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               455.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   142 sec.
    Turnaround time :                            144 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:23:58.168605: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 237
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 2s 100ms/step - loss: 1.2999
Epoch 2/200

16/16 [==============================] - 1s 33ms/step - loss: 1.8092
Epoch 3/200

16/16 [==============================] - 1s 33ms/step - loss: 1.3531
Epoch 4/200

16/16 [==============================] - 1s 33ms/step - loss: 1.1714
Epoch 5/200

16/16 [==============================] - 1s 33ms/step - loss: 1.0353
Epoch 6/200

16/16 [==============================] - 1s 33ms/step - loss: 1.0409
Epoch 7/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9956
Epoch 8/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9988
Epoch 9/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9794
Epoch 10/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9748
Epoch 11/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9713
Epoch 12/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9793
Epoch 13/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9739
Epoch 14/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9716
Epoch 15/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9641
Epoch 16/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9625
Epoch 17/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9618
Epoch 18/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9566
Epoch 19/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9534
Epoch 20/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9518
Epoch 21/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9495
Epoch 22/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9468
Epoch 23/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9459
Epoch 24/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9481
Epoch 25/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9424
Epoch 26/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9435
Epoch 27/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9412
Epoch 28/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9371
Epoch 29/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9345
Epoch 30/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9316
Epoch 31/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9278
Epoch 32/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9250
Epoch 33/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9221
Epoch 34/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9189
Epoch 35/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9147
Epoch 36/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9113
Epoch 37/200

16/16 [==============================] - 1s 33ms/step - loss: 0.9095
Epoch 38/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9045
Epoch 39/200

16/16 [==============================] - 1s 34ms/step - loss: 0.9025
Epoch 40/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8972
Epoch 41/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8967
Epoch 42/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8942
Epoch 43/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8877
Epoch 44/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8954
Epoch 45/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8905
Epoch 46/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8851
Epoch 47/200

16/16 [==============================] - 1s 34ms/step - loss: 0.8846
Epoch 48/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8749
Epoch 49/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8774
Epoch 50/200

16/16 [==============================] - 1s 34ms/step - loss: 0.8836
Epoch 51/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8665
Epoch 52/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8661
Epoch 53/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8680
Epoch 54/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8548
Epoch 55/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8481
Epoch 56/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8481
Epoch 57/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8388
Epoch 58/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8323
Epoch 59/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8266
Epoch 60/200

16/16 [==============================] - 1s 32ms/step - loss: 0.8171
Epoch 61/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8194
Epoch 62/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8295
Epoch 63/200

16/16 [==============================] - 1s 34ms/step - loss: 0.8334
Epoch 64/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8317
Epoch 65/200

16/16 [==============================] - 1s 34ms/step - loss: 0.8201
Epoch 66/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7976
Epoch 67/200

16/16 [==============================] - 1s 33ms/step - loss: 0.8081
Epoch 68/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7991
Epoch 69/200

16/16 [==============================] - 1s 32ms/step - loss: 0.7841
Epoch 70/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7803
Epoch 71/200

16/16 [==============================] - 1s 32ms/step - loss: 0.7680
Epoch 72/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7564
Epoch 73/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7809
Epoch 74/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7966
Epoch 75/200

16/16 [==============================] - 1s 34ms/step - loss: 0.7883
Epoch 76/200

16/16 [==============================] - 1s 34ms/step - loss: 0.7768
Epoch 77/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7741
Epoch 78/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7519
Epoch 79/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7483
Epoch 80/200

16/16 [==============================] - 1s 34ms/step - loss: 0.7368
Epoch 81/200

16/16 [==============================] - 1s 34ms/step - loss: 0.7269
Epoch 82/200

16/16 [==============================] - 1s 34ms/step - loss: 0.7150
Epoch 83/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7008
Epoch 84/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7017
Epoch 85/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6740
Epoch 86/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6757
Epoch 87/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6677
Epoch 88/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6511
Epoch 89/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6472
Epoch 90/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6779
Epoch 91/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6566
Epoch 92/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6725
Epoch 93/200

16/16 [==============================] - 1s 33ms/step - loss: 0.7007
Epoch 94/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6703
Epoch 95/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6556
Epoch 96/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6189
Epoch 97/200

16/16 [==============================] - 1s 33ms/step - loss: 0.6317
Epoch 98/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6194
Epoch 99/200

16/16 [==============================] - 1s 34ms/step - loss: 0.6043
Epoch 100/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5888
Epoch 101/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5888
Epoch 102/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5710
Epoch 103/200

16/16 [==============================] - 1s 34ms/step - loss: 0.5544
Epoch 104/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5401
Epoch 105/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5393
Epoch 106/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5778
Epoch 107/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5962
Epoch 108/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5961
Epoch 109/200

16/16 [==============================] - 1s 34ms/step - loss: 0.5860
Epoch 110/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5648
Epoch 111/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5629
Epoch 112/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5384
Epoch 113/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5477
Epoch 114/200

16/16 [==============================] - 1s 33ms/step - loss: 0.5315
Epoch 115/200

16/16 [==============================] - 1s 34ms/step - loss: 0.5277
Epoch 116/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4998
Epoch 117/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4841
Epoch 118/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4930
Epoch 119/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4831
Epoch 120/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4514
Epoch 121/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4521
Epoch 122/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4538
Epoch 123/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4364
Epoch 124/200

16/16 [==============================] - 1s 34ms/step - loss: 0.4264
Epoch 125/200

16/16 [==============================] - 1s 34ms/step - loss: 0.4336
Epoch 126/200

16/16 [==============================] - 1s 34ms/step - loss: 0.4164
Epoch 127/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4045
Epoch 128/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3968
Epoch 129/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3828
Epoch 130/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3970
Epoch 131/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3869
Epoch 132/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3657
Epoch 133/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3902
Epoch 134/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4834
Epoch 135/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4037
Epoch 136/200

16/16 [==============================] - 1s 33ms/step - loss: 0.4036
Epoch 137/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3858
Epoch 138/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3797
Epoch 139/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3335
Epoch 140/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3499
Epoch 141/200

16/16 [==============================] - 1s 33ms/step - loss: 0.3437
Epoch 142/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3554
Epoch 143/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3540
Epoch 144/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3122
Epoch 145/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3169
Epoch 146/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3006
Epoch 147/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2969
Epoch 148/200

16/16 [==============================] - 1s 34ms/step - loss: 0.3117
Epoch 149/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2823
Epoch 150/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2839
Epoch 151/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2665
Epoch 152/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2651
Epoch 153/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2518
Epoch 154/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2337
Epoch 155/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2288
Epoch 156/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2233
Epoch 157/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2232
Epoch 158/200

16/16 [==============================] - 1s 33ms/step - loss: 0.2239
Epoch 159/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2214
Epoch 160/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2021
Epoch 161/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2112
Epoch 162/200

16/16 [==============================] - 1s 33ms/step - loss: 0.2191
Epoch 163/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2345
Epoch 164/200

16/16 [==============================] - 1s 33ms/step - loss: 0.2243
Epoch 165/200

16/16 [==============================] - 1s 33ms/step - loss: 0.2036
Epoch 166/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2294
Epoch 167/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2348
Epoch 168/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2055
Epoch 169/200

16/16 [==============================] - 1s 33ms/step - loss: 0.2144
Epoch 170/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2154
Epoch 171/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2450
Epoch 172/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2200
Epoch 173/200

16/16 [==============================] - 1s 34ms/step - loss: 0.1975
Epoch 174/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2266
Epoch 175/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1841
Epoch 176/200

16/16 [==============================] - 1s 34ms/step - loss: 0.2052
Epoch 177/200

16/16 [==============================] - 1s 34ms/step - loss: 0.1910
Epoch 178/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1783
Epoch 179/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1607
Epoch 180/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1600
Epoch 181/200

16/16 [==============================] - 1s 32ms/step - loss: 0.1620
Epoch 182/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1545
Epoch 183/200

16/16 [==============================] - 1s 32ms/step - loss: 0.1533
Epoch 184/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1315
Epoch 185/200

16/16 [==============================] - 1s 34ms/step - loss: 0.1217
Epoch 186/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1233
Epoch 187/200

16/16 [==============================] - 1s 34ms/step - loss: 0.1185
Epoch 188/200

16/16 [==============================] - 1s 33ms/step - loss: 0.1104
Epoch 189/200

16/16 [==============================] - 1s 34ms/step - loss: 0.1011
Epoch 190/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0917
Epoch 191/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0902
Epoch 192/200

16/16 [==============================] - 1s 34ms/step - loss: 0.0786
Epoch 193/200

16/16 [==============================] - 1s 34ms/step - loss: 0.0801
Epoch 194/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0704
Epoch 195/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0697
Epoch 196/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0624
Epoch 197/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0590
Epoch 198/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0560
Epoch 199/200

16/16 [==============================] - 1s 33ms/step - loss: 0.0535
Epoch 200/200

16/16 [==============================] - 1s 34ms/step - loss: 0.0491
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 196)          162288    
_________________________________________________________________
activation_1 (Activation)    (None, 334, 196)          0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 334, 196)          784       
_________________________________________________________________
dense_1 (Dense)              (None, 334, 128)          25216     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 334, 128)          512       
_________________________________________________________________
dense_2 (Dense)              (None, 334, 32)           4128      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 32)           0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 334, 32)           128       
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            66        
=================================================================
Total params: 193,122
Trainable params: 192,410
Non-trainable params: 712
_________________________________________________________________
None
Performance training set: 
[0.5122841316389704, 0.5752038659015404, 0.47748215894131507, 0.45429229368607077, 0.5061598557692308, 0.4786742034943474, 0.5088967971530249, 0.48644098644098643, 0.4672340765603772, 0.5819159335288367, 0.5415984724495362, 0.601360544217687, 0.5851893725268513, 0.47549990992613944, 0.4795440821256039, 0.49777582030389783]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.514 (+-0.05), recall: 0.921 (+-0.04), specificity: 0.107, precision: 0.164 (+-0.05) 

	Confusion matrix: 	 [ 496 3993] 
				 [ 64 791]



Performance test set: 
[0.4969491525423729, 0.5163934426229508, 0.4947916666666667]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.503 (+-0.01), recall: 0.903 (+-0.11), specificity: 0.102, precision: 0.204 (+-0.05) 

	Confusion matrix: 	 [ 85 683] 
				 [ 14 180]



Time elapsed: 114.46130728721619
