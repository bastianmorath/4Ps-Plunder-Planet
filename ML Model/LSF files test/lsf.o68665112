Sender: LSF System <lsfadmin@eu-ms-009-25>
Subject: Job 68665112: <python main.py -m 200> in cluster <euler> Done

Job <python main.py -m 200> was submitted from host <eu-ms-009-25> by user <morathba> in cluster <euler> at Sat Jul 14 17:22:22 2018.
Job was executed on host(s) <eu-ms-009-25>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sat Jul 14 17:22:44 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/PP local/Code local/plunder planet/ML Model> was used as the working directory.
Started at Sat Jul 14 17:22:44 2018.
Terminated at Sat Jul 14 17:24:12 2018.
Results reported at Sat Jul 14 17:24:12 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -m 200
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   142.09 sec.
    Max Memory :                                 520 MB
    Average Memory :                             435.67 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               504.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                8
    Run time :                                   105 sec.
    Turnaround time :                            110 sec.

The output (if any) follows:

Using TensorFlow backend.
/cluster/home/morathba/.local/lib64/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
2018-07-14 17:22:55.020399: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Get trained LSTM #################

Maxlen (=Max. #obstacles of logfiles) is 334, minlen is 281
Compiling lstm network...

Shape X: (16, 334, 10)
Shape y: (16, 334, 1)

Epoch 1/200

16/16 [==============================] - 1s 88ms/step - loss: 1.2862
Epoch 2/200

16/16 [==============================] - 0s 24ms/step - loss: 1.8398
Epoch 3/200

16/16 [==============================] - 0s 24ms/step - loss: 1.4298
Epoch 4/200

16/16 [==============================] - 0s 23ms/step - loss: 1.0714
Epoch 5/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9840
Epoch 6/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9971
Epoch 7/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9818
Epoch 8/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9636
Epoch 9/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9653
Epoch 10/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9597
Epoch 11/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9458
Epoch 12/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9412
Epoch 13/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9341
Epoch 14/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9314
Epoch 15/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9261
Epoch 16/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9207
Epoch 17/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9160
Epoch 18/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9138
Epoch 19/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9121
Epoch 20/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9087
Epoch 21/200

16/16 [==============================] - 0s 23ms/step - loss: 0.9042
Epoch 22/200

16/16 [==============================] - 0s 24ms/step - loss: 0.9005
Epoch 23/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8965
Epoch 24/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8955
Epoch 25/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8905
Epoch 26/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8879
Epoch 27/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8835
Epoch 28/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8793
Epoch 29/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8761
Epoch 30/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8717
Epoch 31/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8676
Epoch 32/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8631
Epoch 33/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8603
Epoch 34/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8541
Epoch 35/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8508
Epoch 36/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8455
Epoch 37/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8405
Epoch 38/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8392
Epoch 39/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8340
Epoch 40/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8249
Epoch 41/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8206
Epoch 42/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8221
Epoch 43/200

16/16 [==============================] - 0s 24ms/step - loss: 0.8072
Epoch 44/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8205
Epoch 45/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8218
Epoch 46/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8137
Epoch 47/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8069
Epoch 48/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7975
Epoch 49/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7851
Epoch 50/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7835
Epoch 51/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7805
Epoch 52/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7693
Epoch 53/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7619
Epoch 54/200

16/16 [==============================] - 0s 24ms/step - loss: 0.7649
Epoch 55/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7935
Epoch 56/200

16/16 [==============================] - 0s 23ms/step - loss: 0.8137
Epoch 57/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7756
Epoch 58/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7914
Epoch 59/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7730
Epoch 60/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7712
Epoch 61/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7474
Epoch 62/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7402
Epoch 63/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7324
Epoch 64/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7278
Epoch 65/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7116
Epoch 66/200

16/16 [==============================] - 0s 24ms/step - loss: 0.7025
Epoch 67/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6979
Epoch 68/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6906
Epoch 69/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6840
Epoch 70/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6667
Epoch 71/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6683
Epoch 72/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6584
Epoch 73/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6502
Epoch 74/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6434
Epoch 75/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6360
Epoch 76/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6485
Epoch 77/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6581
Epoch 78/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6304
Epoch 79/200

16/16 [==============================] - 0s 24ms/step - loss: 0.6344
Epoch 80/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6135
Epoch 81/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6099
Epoch 82/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5938
Epoch 83/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5847
Epoch 84/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5714
Epoch 85/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5669
Epoch 86/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5620
Epoch 87/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5663
Epoch 88/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6009
Epoch 89/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5966
Epoch 90/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5604
Epoch 91/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5588
Epoch 92/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5449
Epoch 93/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5210
Epoch 94/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5033
Epoch 95/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5112
Epoch 96/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4994
Epoch 97/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4854
Epoch 98/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4746
Epoch 99/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4704
Epoch 100/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4907
Epoch 101/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5020
Epoch 102/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5170
Epoch 103/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5012
Epoch 104/200

16/16 [==============================] - 0s 24ms/step - loss: 0.4976
Epoch 105/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4909
Epoch 106/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4522
Epoch 107/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4348
Epoch 108/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4474
Epoch 109/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4204
Epoch 110/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4093
Epoch 111/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4061
Epoch 112/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3940
Epoch 113/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3858
Epoch 114/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3853
Epoch 115/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3638
Epoch 116/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3572
Epoch 117/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3449
Epoch 118/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3363
Epoch 119/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3330
Epoch 120/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3369
Epoch 121/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4209
Epoch 122/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6863
Epoch 123/200

16/16 [==============================] - 0s 23ms/step - loss: 0.7023
Epoch 124/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5927
Epoch 125/200

16/16 [==============================] - 0s 23ms/step - loss: 0.6041
Epoch 126/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5522
Epoch 127/200

16/16 [==============================] - 0s 23ms/step - loss: 0.5424
Epoch 128/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4934
Epoch 129/200

16/16 [==============================] - 0s 24ms/step - loss: 0.5156
Epoch 130/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4786
Epoch 131/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4712
Epoch 132/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4270
Epoch 133/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4121
Epoch 134/200

16/16 [==============================] - 0s 23ms/step - loss: 0.4080
Epoch 135/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3928
Epoch 136/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3757
Epoch 137/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3584
Epoch 138/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3481
Epoch 139/200

16/16 [==============================] - 0s 24ms/step - loss: 0.3348
Epoch 140/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3230
Epoch 141/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3127
Epoch 142/200

16/16 [==============================] - 0s 23ms/step - loss: 0.3000
Epoch 143/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2916
Epoch 144/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2807
Epoch 145/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2704
Epoch 146/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2590
Epoch 147/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2515
Epoch 148/200

16/16 [==============================] - 0s 24ms/step - loss: 0.2421
Epoch 149/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2342
Epoch 150/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2238
Epoch 151/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2179
Epoch 152/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2110
Epoch 153/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2079
Epoch 154/200

16/16 [==============================] - 0s 23ms/step - loss: 0.2024
Epoch 155/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1917
Epoch 156/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1818
Epoch 157/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1763
Epoch 158/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1708
Epoch 159/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1648
Epoch 160/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1585
Epoch 161/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1523
Epoch 162/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1462
Epoch 163/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1415
Epoch 164/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1375
Epoch 165/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1333
Epoch 166/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1301
Epoch 167/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1249
Epoch 168/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1189
Epoch 169/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1148
Epoch 170/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1113
Epoch 171/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1092
Epoch 172/200

16/16 [==============================] - 0s 23ms/step - loss: 0.1073
Epoch 173/200

16/16 [==============================] - 0s 24ms/step - loss: 0.1035
Epoch 174/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0981
Epoch 175/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0947
Epoch 176/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0912
Epoch 177/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0902
Epoch 178/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0891
Epoch 179/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0861
Epoch 180/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0846
Epoch 181/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0814
Epoch 182/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0801
Epoch 183/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0757
Epoch 184/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0734
Epoch 185/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0705
Epoch 186/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0703
Epoch 187/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0701
Epoch 188/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0659
Epoch 189/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0638
Epoch 190/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0640
Epoch 191/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0632
Epoch 192/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0601
Epoch 193/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0574
Epoch 194/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0551
Epoch 195/200

16/16 [==============================] - 0s 24ms/step - loss: 0.0548
Epoch 196/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0525
Epoch 197/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0513
Epoch 198/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0494
Epoch 199/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0470
Epoch 200/200

16/16 [==============================] - 0s 23ms/step - loss: 0.0466
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 334, 10)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 334, 128)          71168     
_________________________________________________________________
activation_1 (Activation)    (None, 334, 128)          0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 334, 128)          512       
_________________________________________________________________
dense_1 (Dense)              (None, 334, 96)           12384     
_________________________________________________________________
activation_2 (Activation)    (None, 334, 96)           0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 334, 96)           384       
_________________________________________________________________
dense_2 (Dense)              (None, 334, 64)           6208      
_________________________________________________________________
activation_3 (Activation)    (None, 334, 64)           0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 334, 64)           256       
_________________________________________________________________
time_distributed_1 (TimeDist (None, 334, 2)            130       
=================================================================
Total params: 91,042
Trainable params: 90,466
Non-trainable params: 576
_________________________________________________________________
None
Performance training set: 
[0.5184952978056426, 0.468992663203556, 0.5158301158301158, 0.5449678249678249, 0.5517857142857143, 0.508053508053508, 0.6239088925259137, 0.6446557040082219, 0.4747474747474747, 0.5455473488260374, 0.5849788204626913, 0.47003116758571084, 0.5833804409270775, 0.5592447916666667, 0.6572535991140641, 0.5243402940978984]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.549 (+-0.06), recall: 0.758 (+-0.07), specificity: 0.339, precision: 0.181 (+-0.05) 

	Confusion matrix: 	 [1531 2956] 
				 [202 655]



Performance test set: 
[0.5254633061527056, 0.4900779588944011, 0.517790594498669]


******** Scores for LSTM (Windows:  10, 5, 10) ******** 

	roc_auc: 0.511 (+-0.02), recall: 0.736 (+-0.14), specificity: 0.286, precision: 0.252 (+-0.12) 

	Confusion matrix: 	 [158 478] 
				 [ 60 132]



Time elapsed: 82.42108821868896
