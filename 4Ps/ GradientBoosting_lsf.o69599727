Sender: LSF System <lsfadmin@eu-ms-019-26>
Subject: Job 69599727: <python main.py -t 'Gradient Boosting'> in cluster <euler> Done

Job <python main.py -t 'Gradient Boosting'> was submitted from host <eu-ms-021-18> by user <morathba> in cluster <euler> at Sun Jul 22 15:17:22 2018.
Job was executed on host(s) <eu-ms-019-26>, in queue <normal.4h>, as user <morathba> in cluster <euler> at Sun Jul 22 15:19:14 2018.
</cluster/home/morathba> was used as the home directory.
</cluster/home/morathba/4Ps-Plunder-Planet/4Ps> was used as the working directory.
Started at Sun Jul 22 15:19:14 2018.
Terminated at Sun Jul 22 15:45:35 2018.
Results reported at Sun Jul 22 15:45:35 2018.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py -t 'Gradient Boosting'
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1575.92 sec.
    Max Memory :                                 645 MB
    Average Memory :                             561.23 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               379.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   1600 sec.
    Turnaround time :                            1693 sec.

The output (if any) follows:

Using TensorFlow backend.
Loading dataframes...
Feature matrix already cached!
Feature matrix and labels created!

################# Calculating performance with hyperparameter tuning #################

Doing RandomizedSearchCV with n_iter=10 for Gradient Boosting...
Time elapsed for hyperparameter tuning: 1468.7213537693024
******** Scores of best 3 hyperparameter configurations ********
	Model with rank: 1
		Mean validation score: 0.573 (std: 0.021)
		Parameters: {'learning_rate': 0.2518934728991499, 'loss': 'deviance', 'max_depth': 63, 'max_features': 6, 'min_samples_leaf': 4, 'min_samples_split': 26, 'n_estimators': 1442, 'subsample': 0.410045788029049}
	Model with rank: 2
		Mean validation score: 0.569 (std: 0.023)
		Parameters: {'learning_rate': 0.68023905519525, 'loss': 'exponential', 'max_depth': 40, 'max_features': 5, 'min_samples_leaf': 3, 'min_samples_split': 22, 'n_estimators': 292, 'subsample': 0.027087650334870938}
	Model with rank: 3
		Mean validation score: 0.568 (std: 0.019)
		Parameters: {'learning_rate': 0.38233500081951455, 'loss': 'deviance', 'max_depth': 55, 'max_features': 7, 'min_samples_leaf': 9, 'min_samples_split': 28, 'n_estimators': 2030, 'subsample': 0.6602045264856748}

Calculating performance of Gradient Boosting...

******** Scores for Gradient Boosting (Windows:  10, 5, 10) ******** 

	Hyperparameters: {'loss': 'deviance', 'learning_rate': 0.2518934728991499, 'n_estimators': 1442, 'max_depth': 63, 'min_samples_split': 26, 'min_samples_leaf': 4, 'max_features': 6, 'subsample': 0.410045788029049},
	roc_auc: 0.542 (+-0.04), recall: 0.116 (+-0.06), specificity: 0.891, precision: 0.192 (+-0.04) 

	Confusion matrix: 	 [4258  522] 
				 [901 148]


Time elapsed: 1576.792230606079
